{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4bac3c9-b3fb-42b7-92b0-ac1379884e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a03e6333-3998-4903-91ef-13f8a44faf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5779ad9-ade9-430e-8923-7da27500fe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# create logger with 'Model_application'\n",
    "logger = logging.getLogger('Model')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# create file handler which logs even debug messages\n",
    "fh = logging.FileHandler('ABEdeepoff.log')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "# create formatter and add it to the handlers\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fcca78f-dc04-4896-8430-c8d5b91e5f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zcd/miniconda3/envs/pytorch/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pkbar\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from sklearn.model_selection import GroupShuffleSplit, GroupKFold, train_test_split\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import EarlyStopping\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abae8a99-1b89-4b80-9bf3-34737cd99f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_printoptions(precision=6,sci_mode=False)\n",
    "pd.set_option('display.float_format',lambda x : '%.6f' % x)\n",
    "\n",
    "SEED = 1356\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71eac45e-60ae-4e68-a448-489b54235358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_dir(folder_name=None):\n",
    "    folder_path = os.path.join(os.getcwd(), folder_name)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.mkdir(folder_path)\n",
    "        print(f\"Folder {folder_path} created successfully\")\n",
    "    else:\n",
    "        print(f\"Folder {folder_path} already exists\")\n",
    "    return folder_path\n",
    "\n",
    "\n",
    "def do_encoding(source, target):\n",
    "    '''Sequence encoding.'''\n",
    "    aln = pairwise2.align.globalms(source, target, 1, -1, -3, -2)\n",
    "    src, _aln, tgt = format_alignment(*aln[0]).split('\\n')[:-2]\n",
    "    encode_dict = {'<pad>':0, 'A': 1, 'C': 2, 'G':3, 'T': 4, '-': 5}\n",
    "    seq1 = [encode_dict[nuc] for nuc in src]\n",
    "    seq2 = [encode_dict[nuc] for nuc in tgt]\n",
    "    return source, target, seq1, seq2\n",
    "\n",
    "\n",
    "class gRNADataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        df[['source', 'target', 'seq1', 'seq2']] = df.apply(\n",
    "            lambda x: do_encoding(x['source'], x['target']), axis=1, result_type='expand')\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        self.source = df['source']\n",
    "        self.target = df['target']\n",
    "        self.efficiency = df['efficiency'].values\n",
    "        self.seq1 = df['seq1'].values\n",
    "        self.seq2 = df['seq2'].values\n",
    "        self.otype = df['type']\n",
    "        # print(f'Finished loading the {data} ({df.shape[0]} samples found)')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        source = self.source[index] \n",
    "        target = self.target[index]\n",
    "        y = torch.FloatTensor(np.array(self.efficiency[index]))\n",
    "        seq1 = torch.LongTensor(self.seq1[index])\n",
    "        seq2 = torch.LongTensor(self.seq2[index])\n",
    "        seq_len = seq1.shape[0]\n",
    "        otype = self.otype[index]\n",
    "        return source, target, y, seq1, seq2, seq_len, otype\n",
    "\n",
    "\n",
    "def generate_batch(batch):\n",
    "    ys = []\n",
    "    seqlen_lst = []\n",
    "    source_lst = []\n",
    "    target_lst = []\n",
    "    seq1_lst = []\n",
    "    seq2_lst = []\n",
    "    otype_lst = []\n",
    "    #x[-2]即seq1，他的shape[0]就是seq1的长度（同时也是seq2）的长度\n",
    "    #通过对seq1长度进行排序，可以令每一个batch中都是第一个序列长度最长；\n",
    "    batch = [ (a, b, c, d, e, f, g) for a, b, c, d, e, f, g in sorted( batch, key=lambda x:x[-2], reverse=True) ]\n",
    "\n",
    "    for i, (source, target, y, seq1, seq2, seq_len, otype) in enumerate(batch):\n",
    "        source_lst.append(source)\n",
    "        target_lst.append(target)\n",
    "        ys.append(y)\n",
    "        \n",
    "        seq1_lst.append(seq1)\n",
    "        seq2_lst.append(seq2)\n",
    "        seqlen_lst.append(seq_len)\n",
    "        otype_lst.append(otype)\n",
    "    \n",
    "    # 将序列填充到相同的长度，并设置填充的值为0\n",
    "    padded_seqs = rnn_utils.pad_sequence(seq1_lst, batch_first=False, padding_value=0)\n",
    "    # 对于每个序列，通过 mask 将填充的部分设置为-1\n",
    "    mask = padded_seqs.ne(0)\n",
    "    seq1_batch = padded_seqs.masked_fill(~mask, 0)\n",
    "    \n",
    "    padded_seqs = rnn_utils.pad_sequence(seq2_lst, batch_first=False, padding_value=0)\n",
    "    # 对于每个序列，通过 mask 将填充的部分设置为-1\n",
    "    mask = padded_seqs.ne(0)\n",
    "    seq2_batch = padded_seqs.masked_fill(~mask, 0)\n",
    "    \n",
    "    return (source_lst, target_lst, \n",
    "            torch.FloatTensor(ys),\n",
    "            seq1_batch, seq2_batch,\n",
    "            torch.LongTensor(seqlen_lst), otype_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5129b27-338b-4740-b420-9e2e55d7cd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 模型\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.embedding_1 = nn.Embedding(input_dim, emb_dim)\n",
    "        self.embedding_2 = nn.Embedding(input_dim, emb_dim, _weight=self.embedding_1.weight)\n",
    "        self.rnn = nn.LSTM(input_size=emb_dim, hidden_size=hid_dim, num_layers=n_layers,\n",
    "                           bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)     \n",
    "        self.fc_feat_1 = nn.Linear(6 * hid_dim, 3 * hid_dim)\n",
    "        self.fc_out = nn.Linear(3 * hid_dim, 1) \n",
    "        self.att_score = None\n",
    "    \n",
    "    def attention_net(self, x, query, mask=None): \n",
    "        \n",
    "        d_k = query.size(-1)\n",
    "        scores = torch.matmul(query, x.transpose(1, 2)) / math.sqrt(d_k)  \n",
    "        alpha_n = F.softmax(scores, dim=-1) \n",
    "        context = torch.matmul(alpha_n, x).sum(1)\n",
    "        return context, alpha_n\n",
    "    \n",
    "    def forward(self,seq_1, seq_2):\n",
    "        emb_1 = self.embedding_1(seq_1)\n",
    "        emb_2 = self.embedding_2(seq_2)\n",
    "        emb_comb = self.dropout(emb_1 + emb_2)\n",
    "        #self.embedding = emb_comb\n",
    "        out, (hid_, _) = self.rnn(emb_comb)\n",
    "        hidden = torch.cat( (hid_[-2,:,:], hid_[-1,:,:]), dim = 1 )\n",
    "        \n",
    "        out = out.permute(1,0,2)\n",
    "        avg_pool = torch.mean( out, 1)\n",
    "        max_pool, _ = torch.max( out, 1)\n",
    "                             \n",
    "        query = self.dropout(out)\n",
    "        # 加入attention机制\n",
    "        attn_output, alpha_n = self.attention_net(out, query)\n",
    "        self.att_score = alpha_n\n",
    "        \n",
    "        #hid_size*2*3\n",
    "        x = torch.cat([ attn_output, hidden, max_pool], dim=1)\n",
    "        x = self.dropout(F.relu(self.fc_feat_1( x )))\n",
    "        fc_out = self.fc_out(x)\n",
    "        return fc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "807db23a-660e-4d27-ba0a-256b7894b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_external = pd.read_table('data/ABE_Off_endo.txt', skip_blank_lines=True)\n",
    "df_external.dropna(inplace=True)\n",
    "df_external['source'] = df_external.seq1.str.upper().str.strip()\n",
    "df_external['target'] = df_external.seq2.str.upper().str.strip().str.replace('-','')\n",
    "df_external['target_len'] = df_external.target.apply(len)\n",
    "df_external['efficiency'] = df_external.y\n",
    "df_external['type'] = df_external['off_type']\n",
    "df_on = df_external[df_external['type'] == 'Y'].copy()\n",
    "df_on['oneff'] = df_on.efficiency\n",
    "df_external = df_external.merge(df_on[['source', 'Group', 'oneff']])\n",
    "df_external['off/on'] = df_external.efficiency/df_external.oneff\n",
    "df_external['efficiency'] = df_external['off/on']\n",
    "df_external.drop_duplicates(inplace=True)\n",
    "df_external.reset_index(drop=True,inplace=True)\n",
    "df_external_dataset = gRNADataset(df_external)   \n",
    "exter_iter =  DataLoader(df_external_dataset, batch_size=5, shuffle=False, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "addd6c5c-bd95-4c04-a258-9e4784c268cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_iter, valid_iter, test_iter, patience, k, version=None):\n",
    "    global my_optim\n",
    "    global lr_dict\n",
    "    global lst_testing\n",
    "    global lst_endo\n",
    "    N_EPOCHS = 250\n",
    "    CLIP = 1\n",
    "    best_valid_loss = float('inf')\n",
    "    # to track the training loss as the model trains\n",
    "    train_losses = []\n",
    "    # to track the validation loss as the model trains\n",
    "    valid_losses = []\n",
    "    # to track the average training loss per epoch as the model trains\n",
    "    avg_train_losses = []\n",
    "    # to track the average validation loss per epoch as the model trains\n",
    "    avg_valid_losses = [] \n",
    "    \n",
    "    path = safe_dir(version)\n",
    "    # initialize the early_stopping object\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True, path=f'{path}/ABE_RNN_{k}_checkpoint.pt')\n",
    "    early_num = 0\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        train_per_epoch = len(train_iter)\n",
    "        kbar = pkbar.Kbar(target=train_per_epoch, epoch=epoch, num_epochs=N_EPOCHS, width=8, always_stateful=False)\n",
    "\n",
    "        model.train()\n",
    "        epoch_loss_ = 0\n",
    "        \n",
    "        for i, batch in enumerate(train_iter):\n",
    "            seq1 = batch[3].to(device)\n",
    "            seq2 = batch[4].to(device)\n",
    "            y = batch[2].unsqueeze(1).to(device)\n",
    "            length = batch[5].to(device)\n",
    "            wgt_dict = (1/((pd.Series(batch[-1]).value_counts()/(pd.Series(batch[-1]).value_counts().sum()))+0.08)).to_dict()\n",
    "            wgts_lst = [wgt_dict[x] for x in batch[-1]]\n",
    "            wgts = torch.FloatTensor(wgts_lst).to(device)\n",
    "            \n",
    "\n",
    "            my_optim.zero_grad()\n",
    "            outputs = model(seq1, seq2)\n",
    "\n",
    "            #src_y regression\n",
    "            y_hat = torch.sigmoid(outputs) * 100\n",
    "            loss = (criterion_mse(y, y_hat) * wgts).mean()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "            \n",
    "            my_optim.step()\n",
    "            \n",
    "            epoch_loss_ += loss.item()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            kbar.update(i, values=[(\"lr\", my_optim.defaults['lr']), \n",
    "                (\"loss\", epoch_loss_ / (i + 1))])\n",
    "        \n",
    "        model.eval()\n",
    "        epoch_loss_ = 0\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for i, batch in enumerate(valid_iter):\n",
    "\n",
    "                seq1 = batch[3].to(device)\n",
    "                seq2 = batch[4].to(device)\n",
    "                y = batch[2].unsqueeze(1).to(device)\n",
    "                length = batch[5].to(device)\n",
    "                #wgts = batch[2].to(device)\n",
    "                wgt_dict = (1/((pd.Series(batch[-1]).value_counts()/(pd.Series(batch[-1]).value_counts().sum()))+0.08)).to_dict()\n",
    "                wgts_lst = [wgt_dict[x] for x in batch[-1]]\n",
    "                wgts = torch.FloatTensor(wgts_lst).to(device)\n",
    "                my_optim.zero_grad()\n",
    "                outputs = model(seq1, seq2)\n",
    "\n",
    "                y_hat = torch.sigmoid(outputs) * 100\n",
    "                loss = (criterion_mse(y, y_hat)).mean()\n",
    "\n",
    "                valid_losses.append(loss.item())\n",
    "                epoch_loss_ += loss.item()\n",
    "\n",
    "            kbar.add(1, values=[(\"val_loss\", epoch_loss_ / len(valid_iter))])\n",
    "\n",
    "        valid_loss = epoch_loss_ / len(valid_iter)\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            p = f'{path}/ABE_RNN_{k}_{round(valid_loss, 6)}.pt'\n",
    "            torch.save(model.state_dict(), p)\n",
    "            logger.info(f'----Testing------,{p}')\n",
    "            r = get_test(test_iter, model)\n",
    "            lst_testing.append([p,r])\n",
    "            logger.info(f'----Endo------')\n",
    "            r = get_endo(exter_iter, model, df_external)\n",
    "            lst_endo.append([p,r])\n",
    "\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "\n",
    "        # clear lists to track next epoch\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "\n",
    "        # early_stopping needs the validation loss to check if it has decresed, \n",
    "        # and if it has, it will make a RNN_checkpoint of the current model\n",
    "        early_stopping(valid_loss, model, p)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            early_num = early_num + 1\n",
    "            ##最后一个，换learning rate\n",
    "            if early_num == 4:\n",
    "                print(\"Early stopping with best_valid_loss:\", epoch, best_valid_loss, \". Going next...\")\n",
    "                model.load_state_dict(torch.load(p))\n",
    "                my_optim = optim.Adam(model.parameters(),lr=lr_dict[0])\n",
    "                early_stopping = EarlyStopping(patience=patience, verbose=True, \n",
    "                    path=f'{path}/ABE_RNN_{k}_checkpoint.pt')\n",
    "                break\n",
    "                \n",
    "            print(\"Change learning rate..\")\n",
    "            model.load_state_dict(torch.load(p))\n",
    "            my_optim = optim.Adam(model.parameters(),lr=lr_dict[early_num]) \n",
    "            early_stopping.early_stop = False\n",
    "            early_stopping.counter = 0\n",
    "            early_stopping = EarlyStopping(patience=patience, verbose=True, best_score = -best_valid_loss,\n",
    "                path=f'{path}/ABE_RNN_{k}_checkpoint.pt')\n",
    "                \n",
    "            \n",
    "def get_pred(iter_, model):\n",
    "    model.eval()\n",
    "    lst_dfs = []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iter_):\n",
    "            seq1 = batch[3].to(device)\n",
    "            seq2 = batch[4].to(device)\n",
    "            y = batch[2].unsqueeze(1).to(device)\n",
    "            length = batch[5].to(device)\n",
    "\n",
    "            out_eff = model(seq1, seq2)\n",
    "            y = list(y.view(-1).cpu().numpy() / 100)\n",
    "            out_eff = torch.sigmoid(out_eff)\n",
    "            out_eff = list(out_eff.view(-1).cpu().numpy())\n",
    "\n",
    "            df_gRNA = pd.DataFrame({'source': batch[0],'target': batch[1], \n",
    "                'offtype': batch[-1]})\n",
    "            df_gRNA['y'] = y\n",
    "            df_gRNA['y_pred'] = out_eff\n",
    "            lst_dfs.append(df_gRNA)\n",
    "    df_conc = pd.concat(lst_dfs)\n",
    "    return df_conc\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'rnn.weight_' in name:\n",
    "            nn.init.orthogonal_(param.data)\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3616bac-e1ae-470d-8fea-198ef0d74f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/ABEdeepoff.txt', sep='\\t')\n",
    "df['efficiency'] = df['efficiency'] * 100\n",
    "\n",
    "i = 1\n",
    "df_list = []\n",
    "for src, sub_df in df.groupby('source', sort=False):\n",
    "    sub_df['group'] = i\n",
    "    i += 1\n",
    "    df_list.append(sub_df)\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "df_src = df.copy().sample(frac=1,random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1800fb8-fb11-4c98-9622-d680dcb3d2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_on = df_src[df_src['type'] == 'Y'].copy()\n",
    "df_on['oneff'] = df_on.efficiency\n",
    "df_src = df_src.merge(df_on[['source','oneff']])\n",
    "df_src['off/on'] = df_src.efficiency/df_src.oneff\n",
    "df_src['efficiency'] = df_src['off/on'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04dca3f4-8e2c-4a7d-bbc2-cc08459c9b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test(iter_data, model):\n",
    "    df_ = get_pred(iter_data, model)\n",
    "    grp = df_.groupby('offtype')\n",
    "    \n",
    "    all_corr = df_.corr(method='spearman').y[-1]\n",
    "    logger.info(f\"all_corr:{all_corr}\")\n",
    "    lst_r = []\n",
    "    for k in grp.groups.keys():\n",
    "        df_grp = grp.get_group(k)\n",
    "        r = df_grp.corr(method='spearman').y[-1]\n",
    "        lst_r.append(f'{k},{r}')\n",
    "        logger.info(f\"{k}:{r}\")\n",
    "    return all_corr,lst_r\n",
    "\n",
    "\n",
    "def get_endo(iter_data, model,df_external):\n",
    "    df_ = get_pred(iter_data,model)\n",
    "    df_['group'] = df_external['Group'].values\n",
    "\n",
    "    grp = df_.groupby('group')\n",
    "    all_corr = df_.corr(method='spearman').y[1]\n",
    "    logger.info(f\"all_corr:{all_corr}\")\n",
    "    lst_r = []\n",
    "    for k in grp.groups.keys():\n",
    "        df_grp = grp.get_group(k)\n",
    "        r = df_grp.corr(method='spearman').y[1]\n",
    "        lst_r.append(f'{k},{r}')\n",
    "        logger.info(f\"{k}:{r}\")\n",
    "    return lst_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d795b8f1-9278-4ba8-8b79-be115b1b44f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENC_INPUT_DIM = 6\n",
    "ENC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "model = Encoder(ENC_INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, N_LAYERS, ENC_DROPOUT).to(device)\n",
    "\n",
    "lst_testing = []\n",
    "lst_endo = []\n",
    "lr_dict = {0: 0.001, 1: 0.0001, 2: 0.00001, 3: 0.000005}\n",
    "criterion_mse = nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c790eac-1d71-4d47-a532-760da5208c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = GroupKFold(n_splits=10)\n",
    "splits = kf.split(df_src, groups=df_src['group'])\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(splits):\n",
    "    data_train, data_test = df_src.loc[train_idx], df_src.loc[test_idx]\n",
    "    test_dataset = gRNADataset(data_test)\n",
    "    test_iter = DataLoader(test_dataset, batch_size=128, shuffle=False, collate_fn=generate_batch)\n",
    "    \n",
    "    data_train.reset_index(drop=True, inplace=True)\n",
    "    data_train, data_valid = train_test_split(data_train, train_size=0.9, random_state=SEED)\n",
    "    \n",
    "    train_dataset = gRNADataset(data_train)\n",
    "    train_iter = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=generate_batch)\n",
    "\n",
    "    valid_dataset = gRNADataset(data_valid)\n",
    "    valid_iter = DataLoader(valid_dataset, batch_size=32, shuffle=True, collate_fn=generate_batch)\n",
    "\n",
    "    my_optim = optim.AdamW(model.parameters(), lr=lr_dict[0])\n",
    "    model.apply(init_weights)\n",
    "    train_model(train_iter, valid_iter, test_iter, 5, i, version='ABEdeepoff_0525')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7c71c7-7731-4a4f-bf61-5a08975c010e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
